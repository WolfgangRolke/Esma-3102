---
header-includes: \usepackage{color}
output:
  html_document: default
  pdf_document:
    fig_caption: no
---
<style>
table, th, td { text-align:right; }
th, td {padding: 10px;}
</style>

```{r, echo=FALSE, warning=FALSE, message=FALSE}
whichcomp <- strsplit(getwd(),"/")[[1]][3]
load(paste0("c:/users/", whichcomp, "/Dropbox/teaching/Resma3/Resma3.RData"))
library(knitr)
opts_chunk$set(fig.width=6, fig.align = "center", 
      out.width = "70%", warning=FALSE, message=FALSE)
library(ggplot2)
library(grid)
```
`r hl()$basefontsize()`

#Non-Normal Residuals, No Equal Variance - Transformations

##Categorical - Quantitative

###Case Study: Cancer Survival

As we saw before, the boxplot of this data shows some severe outliers: 
```{r}
attach(cancersurvival)
head(cancersurvival)
bplot(Survival, Cancer)
```

These are often an indication that there is a problem with the assumption of normaly distributed residuals. In fact, when we run the ANOVA and check the normal plot we can see that this is the case:

```{r}
oneway(Survival,Cancer)
```

So, what can we do? One possible solution is to use a **log transformation**:

```{r}
bplot(log(Survival), Cancer)
```

This takes care of (most) of the outliers.

Outliers often have another effect:
   
```{r}
stat.table(Survival, Cancer) 
```

shows we also have a problem with the equal variance: smallest stdev=210, largest stdev=1239, 3*210=630 < 1239.

`r hl()$hr()`

In this class we will use the log transform only. In real live there are a number of other transforms one can try, such as square root and inverse. 

**Note** sometimes in a quantitative variable some values are 0, but log(0) does not exist!. In this case use log(x+1). Even worse, sometimes numbers are negative, and again log(negative number) does not exist. In that case use log(x+a) so that all x+a>0

`r hl()$hr()`

We already know that outliers have a strong effect on the mean and the standard deviation. It might therefore be better to use a summary table based on median and iqr:

```{r}
stat.table(Survival, Cancer, Mean=FALSE)
```

Now we can finish the analysis of this dataset:

```{r}
oneway(log(Survival),Cancer) 
```

1. Parameters of interest: group means  
2. Method of analysis: ANOVA  
3. Assumptions of Method: residuals have a normal distribution, groups have equal variance  
4.  $\alpha = 0.05$  
5. Null hypothesis H~0~: $\mu_1 = \mu_2 = \mu_3 = \mu_4 = \mu_5$ (groups have the same means) 
6. Alternative hypothesis H~a~: $\mu_i \ne \mu_j$ (at least two groups have different means)  
7. p value = 0.0041  
8. 0.0041 < 0.05, there is some evidence that the group means are not the same, there are differences in the survival times. 
Assumptions:
a normal plot of residuals ok   
b smallest stdev = 1.0, largest stdev = 1.6, 3*1.0 = 3.0 > 1.6 ok
 
Notice that the transformation solves both the problem of the normal residuals as well as the problem of unequal variances! This is quite often the case, though not always.

###Case Study: Capacity of Wells

The specific capacity of wells in the Appalachian mountain region of Pennsylvania has been measured in four rock types. (Knopman 1990) The rock types are dolomite, limestone, siliclastic and metamorphic. The capacities are recorded in gal/min/ft.
  
```{r}
attach(rocks)
head(rocks)
table(Rocks)
bplot(Capacity, Rocks)
```

Clearly there some serious outliers. Let's try the log transform: 
  
```{r}
bplot(log(Capacity), Rocks)  
```
and this looks much better. 

**Summary Statistics**

Because we used a transformation we will use the median and IQR

```{r}
stat.table(Capacity, Rocks, Mean=FALSE)
```

Note that the estimates of the variation differ by quite a lot (1.0 vs 9.1). This again is due to the fact that we have many outliers in the dataset.

Now the test:

```{r}
oneway(log(Capacity), Rocks)
```

1.   Parameters of interest: group means  
2.   Method of analysis: ANOVA  
3.   Assumptions of Method: residuals have a normal distribution, groups have equal variance  
4.   $\alpha = 0.05$   
5.   H~0~: $\mu_1 = \mu_2 = \mu_3 = \mu_4$ (no difference in the mean Capacity for different Rocks)   
6.   H~a~: $\mu_i \ne \mu_j$ for some i and j (some differences in the mean Capacitys for different Rocks)    
7.   p-value = 0.0067   
8.   p < $\alpha$, we reject H~0~, there are some differences in the mean Capacity for different Rocks.

Assumptions: Normal plot looks ok   

smallest stdev of log(Capacity): 0.61, largest stdev: 1.11, 3*0.61 = 18.3 > 1.11, ok  

`r hl()$fontcolor("Warning")`

If we had not done a transformation the results would have been quite different. For example, rocks would not have been stat. significant (p-value = 0.06)

##Quantitative - Quantitative

### Case Study: Brain and Body Weight of 62 Mammals

Brain and Body Weight (in kg) of 62 Mammals.  

```{r}
head(brainsize)
```

We have two quantitative variables, so we should start with the scatterplot:

```{r}
attach(brainsize)
brainsize
splot(brain.wt.g, body.wt.kg)
```

unfortunately almost all the "space" in the graph is taken up by a few outliers, it is not even possible to determine if there is a relationship between the variables. Drawing the marginal plot show that the problem are outliers in both variables:

```{r, warning=FALSE}
mplot(brain.wt.g, body.wt.kg)
```

As before we can try and fix this problem by using a **log transformation**: 

```{r, warning=FALSE}
mplot(log(brain.wt.g), log(body.wt.kg))
```

which nicely fixes the problem. 

`r hl()$hr()`

Because now we have two quantitative variables the log transform could be applied to x, to y or to both. In general we might see any of these combinations:

```{r, echo=FALSE, warning=FALSE}
x <- runif(100, 0, 10)
y <- 10*x+rnorm(100, 0, 10)
mplot(y, x)
```

$\rightarrow$ no transformations needed

`r hl()$hr()`

```{r, echo=FALSE, warning=FALSE}
x1 <- runif(100, 0, 10)
y <- 10*x1+rnorm(100, 0, 10)
x <- exp(x1)
```
```{r, warning=FALSE}
mplot(y, x)
```

x variable is bad, y variable is ok

$\rightarrow$ log transform x, leave y alone:

```{r, warning=FALSE}
mplot(y, log(x))
```

`r hl()$hr()`

```{r, echo=FALSE, warning=FALSE}
x <- runif(100, 0, 10)
y1 <- 1*x+rnorm(100, 0, 1)
y <- exp(y1)
```

```{r, warning=FALSE}
mplot(y, x)
```

y variable is bad, x variable is ok

$\rightarrow$ log transform y, leave x alone:

```{r, warning=FALSE}
mplot(log(y), x)
```

`r hl()$hr()`

```{r, echo=FALSE, warning=FALSE}
x1 <- runif(100, 0, 10)
y1 <- x1+rnorm(100, 0, 1)
x <- exp(x1)
y <- exp(y1)
```

```{r, warning=FALSE}
mplot(y, x)
```

both x and y variables are bad

$\rightarrow$ log transform x and y

```{r, warning=FALSE}
mplot(log(y), log(x))
```

`r hl()$hr()`

`r hl()$hr()`

It is clear from the scatterplot that we have a strong linear relationship between log(Brain) and log(Body), but if we want to we can now also find Pearson's correlation coefficient: 

```{r, warning=FALSE}
cor(log(body.wt.kg), log(brain.wt.g))
```

Doing so for the original data would have been wrong! 


```{r, echo=FALSE}
detach(cancersurvival)
detach(rocks)
detach(brainsize)
```
